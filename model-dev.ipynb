{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy settings\n",
    "np.random.seed(1)\n",
    "np.set_printoptions(suppress=True, linewidth = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  64 128 128 128 255 255 255 255 255 191  64   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  64 255 255 255 255 255 255 255 255 255 255 255 191   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 128 255 255 255 255 255 255 255 255 255 255 255 255  64   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  64 255 255 191 128 128   0 128 128 128 191 255 255 191   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 255 255 255 255   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  64 191 255 255 255 191   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 191 255 255 255 128   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  64 128 255 255 255 255 128   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  64 255 255 255 255 255 255 255   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  64 255 255 255 255 255 255 255 255 255 128  64   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 128 255 255 255 255 255 255 255 255 255 255 255 191   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  64 128 128   0 191 128 128 128 255 255 255 255 191   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 128   0   0   0   0   0   0   0 128 255 255 255  64   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  64 191  64   0   0   0   0   0   0   0   0 128 255 255 255   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 255 191   0   0   0   0   0   0   0   0   0  64 255 255 255 191   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 255 128   0   0   0   0   0   0   0   0 191 255 255 255 255   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 128 255 128   0   0   0   0  64 128 255 255 255 255 255 191  64   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  64 255 255 191 128 255 255 255 255 255 255 255 255 191   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 191 255 255 255 255 255 255 255 255 255 191  64   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  64 128 128 255 255 128 128 128   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "3\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "from dataload import read_images_labels\n",
    "\n",
    "k = 10 # k-hot value\n",
    "\n",
    "# load train\n",
    "training_images_filepath = './data/train-images-idx3-ubyte/train-images-idx3-ubyte'\n",
    "training_labels_filepath = './data/train-labels-idx1-ubyte/train-labels-idx1-ubyte'\n",
    "x_train, y_train = read_images_labels(training_images_filepath, training_labels_filepath)\n",
    "\n",
    "# load test\n",
    "test_images_filepath = './data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte'\n",
    "test_labels_filepath = './data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte'\n",
    "x_test, y_test = read_images_labels(test_images_filepath, test_labels_filepath)\n",
    "\n",
    "print(np.array(x_train[45854]).reshape(28,28))\n",
    "print(np.array(y_train[45854]))\n",
    "\n",
    "# reformat data for model\n",
    "x_train = x_train.transpose() * (1/255)\n",
    "\n",
    "# data dimension D\n",
    "D = x_train.shape[0]\n",
    "assert D==784\n",
    "# number of training examples N\n",
    "N = x_train.shape[1]\n",
    "\n",
    "# reformat data to k-hot format TODO: does numpy have a better way to do this?\n",
    "temp_array = np.zeros((k, N))\n",
    "for index,val in enumerate(y_train):\n",
    "    temp_array[val][index] = 1\n",
    "print(temp_array[:, 45854])\n",
    "y_train = temp_array\n",
    "\n",
    "x_test = x_test.transpose() * (1/255)\n",
    "y_test = y_test.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2508/1994746260.py:17: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  x_train = data[[range(iris_dataset.shape[0]//5*3)]]\n",
      "/tmp/ipykernel_2508/1994746260.py:19: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  x_test = data[[range(iris_dataset.shape[0]//5*3, iris_dataset.shape[0])]]\n",
      "/tmp/ipykernel_2508/1994746260.py:34: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  labels_train = labels[[range(iris_dataset.shape[0]//5*3)]]\n"
     ]
    }
   ],
   "source": [
    "# iris dataset\n",
    "\n",
    "k = 3 # k-hot value\n",
    "\n",
    "# sepal_length,sepal_width,petal_length,petal_width,species\n",
    "iris_dataset = np.genfromtxt('iris.csv', delimiter=',')\n",
    "\n",
    "# randomize and split\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(iris_dataset)\n",
    "\n",
    "data = iris_dataset[:, [range(4)]]\n",
    "labels = iris_dataset[:, 4]\n",
    "\n",
    "data = np.reshape(data, (150,4))\n",
    "\n",
    "x_train = data[[range(iris_dataset.shape[0]//5*3)]]\n",
    "x_train = x_train.transpose()\n",
    "x_test = data[[range(iris_dataset.shape[0]//5*3, iris_dataset.shape[0])]]\n",
    "x_test = x_test.transpose()\n",
    "\n",
    "# data dimension D\n",
    "D = x_train.shape[0]\n",
    "assert D==4\n",
    "# number of training examples N\n",
    "N = x_train.shape[1]\n",
    "\n",
    "labels_train = labels[[range(iris_dataset.shape[0]//5*3)]]\n",
    "\n",
    "# format\n",
    "# reformat data to k-hot format TODO: does numpy have a better way to do this?\n",
    "y_train = np.zeros((k, N))\n",
    "for index, val in enumerate(labels_train): y_train[int(val)][index] = 1\n",
    "\n",
    "y_test = labels[iris_dataset.shape[0]//5*3:iris_dataset.shape[0]].reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.03712917766447455\n",
      "Cost at epoch 100: 0.007377760365864374\n",
      "Cost at epoch 200: 0.007175502898641945\n",
      "Cost at epoch 300: 0.007185704615716157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2508/1822056111.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mz_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0ma_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mz_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### instatiate model architecture\n",
    "\n",
    "# architecture hyperparameters\n",
    "hidden_dim = 15 # width of hidden layer\n",
    "\n",
    "# instatiate first matrix of weights\n",
    "w_2 = np.random.normal(loc=0, scale=1, size=(hidden_dim,D))\n",
    "# instatiate first bias vector\n",
    "b_2 = np.random.normal(loc=0, scale=1, size=(hidden_dim,1))\n",
    "\n",
    "# instatiate second set of weights\n",
    "w_3 = np.random.normal(loc=0, scale=1, size=(k,hidden_dim))\n",
    "# instatiate second set of weights\n",
    "b_3 = np.random.normal(loc=0, scale=1, size=(k,1))\n",
    "\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x): \n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1-sig)\n",
    "\n",
    "# implementation of softmax from: https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python \n",
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "### training\n",
    "\n",
    "# training hyperparamters\n",
    "alpha = 0.1 # learning rate\n",
    "epochs = 300 # number of epochs\n",
    "batch_size = 5\n",
    "\n",
    "def extrem(array, array_name): return f'Extremal Values for {array_name}: {np.amin(array), np.amax(array)}'\n",
    "\n",
    "def loss(activation, label): return 0.5*(activation-label)**2\n",
    "\n",
    "def cost(loss_vec): return np.average(loss_vec)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_index in range(int(N/batch_size)): # look at this again\n",
    "        x_train_batch = x_train[:, [batch_index, batch_index + batch_size]] \n",
    "        y_train_batch = y_train[:, [batch_index, batch_index + batch_size]]\n",
    "\n",
    "        # forward pass\n",
    "        z_2 = w_2.dot(x_train_batch) + b_2.dot(np.ones((1,x_train_batch.shape[1])))\n",
    "        a_2 = sigmoid(z_2) \n",
    "        z_3 = w_3.dot(a_2) + b_3.dot(np.ones((1,a_2.shape[1])))\n",
    "        a_3 = sigmoid(z_3)\n",
    "\n",
    "        # backprop\n",
    "        delta_3 = np.multiply((a_3-y_train_batch), sigmoid_prime(z_3))\n",
    "        delta_2 = np.multiply(w_3.transpose().dot(delta_3), sigmoid_prime(z_2))\n",
    "        # print(delta_2.shape)\n",
    "        # print(x_train_batch.transpose().shape)\n",
    "\n",
    "        # FULL gradient descent TODO: stochastic\n",
    "        # make consistent layer labelling! Nielsen and Gross used different indexing for layer number\n",
    "        w_3 -= alpha*(1/batch_size)*delta_3.dot(a_2.transpose())\n",
    "        b_3 -= alpha*np.reshape(np.mean(delta_3, axis=1), (-1,1))\n",
    "\n",
    "\n",
    "        w_2 -= alpha*(1/batch_size)*delta_2.dot(x_train_batch.transpose())\n",
    "        b_2 -= alpha*np.reshape(np.mean(delta_2, axis=1), (-1,1))\n",
    "        # if batch_index % 10 == 0: print(f\"Batch {batch_index} completed.\")\n",
    "\n",
    "    # final foward pass\n",
    "    a_2 = sigmoid(w_2.dot(x_train) + b_2.dot(np.ones((1,x_train.shape[1]))))\n",
    "    a_3 = sigmoid(w_3.dot(a_2) + b_3.dot(np.ones((1,x_train.shape[1]))))\n",
    "\n",
    "    loss_val = loss(a_3, y_train)\n",
    "    cost_val = cost(loss_val)\n",
    "    if epoch % 100 == 0: print(f\"Cost at epoch {epoch}: {cost_val}\")\n",
    "    # print(f\"Cost at epoch {epoch}: {cost_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968 of 10000 wrong \n",
      "Accuracy: 0.9032\n"
     ]
    }
   ],
   "source": [
    "### testing\n",
    "\n",
    "# test dimension D_test\n",
    "D_test = x_test.shape[1]\n",
    "\n",
    "test_ones_1 = np.ones((1,D_test))\n",
    "test_ones_2 = np.ones((1,D_test))\n",
    "\n",
    "# forward pass on test data\n",
    "test_a_2 = sigmoid(w_2.dot(x_test) + b_2.dot(np.ones((1,x_test.shape[1]))))\n",
    "test_a_3 = sigmoid(w_3.dot(test_a_2) + b_3.dot(np.ones((1,x_test.shape[1]))))\n",
    "predictions = np.argmax(test_a_3, axis=0)\n",
    "\n",
    "num_wrong = np.sum(np.not_equal(y_test, predictions))\n",
    "test_size = y_test.shape[1]\n",
    "print(f\"{num_wrong} of {test_size} wrong \\nAccuracy: {1 - (num_wrong/test_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare to\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,hidden_dim, dtype=torch.float64),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(hidden_dim,10, dtype=torch.float64),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train.transpose())\n",
    "y_train_tensor = torch.from_numpy(y_train.transpose())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(x_train_tensor)\n",
    "    loss = loss_fn(y_pred, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "x_test_tensor = torch.from_numpy(x_test.transpose())\n",
    "\n",
    "y_test_t = model(x_test_tensor)\n",
    "predictions = torch.argmax(y_test_t, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
